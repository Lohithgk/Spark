{"cells":[{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/Superstore_Sales.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark\\\n    .read\\\n    .format(file_type)\\\n    .option(\"inferSchema\", infer_schema)\\\n    .option(\"header\", first_row_is_header)\\\n    .option(\"sep\", delimiter)\\\n    .load(file_location)\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6482be4c-f067-47c9-b0ac-35c938b94601"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Create a view or table\ndf.createOrReplaceTempView(\"Superstore_Sales_csv\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd82bb99-1479-4d5c-be10-8c36df0f1d44"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\n\n/* Query the created temp table in a SQL cell */\n\nselect * from `Superstore_Sales_csv`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b5f66379-6f7f-42ec-8e82-d0e0926a1721"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.\n# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.\n# To do so, choose your table name and uncomment the bottom line.\n\npermanent_table_name = \"Superstore_Sales_csv\"\n\n# df.write.format(\"parquet\").saveAsTable(permanent_table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"db9631f6-bb4a-42ca-8a3c-0d48af932331"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1 = spark.read\\\n    .format(\"csv\")\\\n    .option(\"header\", True)\\\n    .option(\"inferSchema\", True)\\\n    .option(\"path\", \"/FileStore/tables/Superstore_Sales.csv\")\\\n    .load()\n\ndf1.orderBy(\"COUNTRY\")\\\n   .groupBy(\"COUNTRY\")\\\n   .agg(expr(\"SUM(QUANTITYORDERED) as Total_Quantity\"),\n        expr(\"SUM(SALES) as Total_Sales\"))\\\n   .where(col(\"COUNTRY\")==\"Australia\")\\\n   .show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa1403a9-624b-4f2e-ab20-cf60271e94a4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Input data - List\nmyList = [(1,\"2013-07-25\",11599,\"CLOSED\"),\n          (2,\"2014-07-25\",256,\"PENDING_PAYMENT\"),\n          (3,\"2013-07-25\",11599,\"COMPLETE\"),\n          (4,\"2019-07-25\",8827,\"CLOSED\")]\n\ndf1=spark.createDataFrame(myList)\\\n    .toDF(\"orderid\",\"orderdate\",\"customerid\",\"status\")\n\nnewDf = df1\\\n    .withColumn(\"date1\",to_timestamp(col(\"orderdate\")))\\\n    .withColumn(\"newid\",monotonically_increasing_id())\\\n    .dropDuplicates([\"orderdate\",\"customerid\"])\\\n    .sort(\"orderdate\")\n\ndf1.show()\nnewDf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42e93846-afcf-4ab7-8345-932cc1d5c1cf"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Input data - List\nmyList = [(1,\"2013-07-25\",11599,\"CLOSED\"),\n          (2,\"2014-07-25\",256,\"PENDING_PAYMENT\"),\n          (3,\"2013-07-25\",11599,\"COMPLETE\"),\n          (4,\"2019-07-25\",8827,\"CLOSED\")]\n\nspark.createDataFrame(myList).show()\n\ndf1=spark.createDataFrame(myList)\\\n    .toDF(\"orderid\",\"orderdate\",\"customerid\",\"status\").show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ea14b8e-5719-40f4-909b-7aa3b13265bd"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n# Sample data\nmyList = [(\"anusha\",16,\"Mysore\"),\n          (\"narendra\",22,\"Hydrabad\"),\n          (\"ankit\",25,\"Toronto\"),\n          (\"suchet\",14,\"Bangalore\")]\n\n# Create dataframe from data source Mylist\ndf1= spark.createDataFrame(myList).toDF(\"name\",\"age\",\"city\")\n\n# ageCheck User defined Function is created.\ndef ageCheck(age):\n    if(age > 18):\n      return \"Y\"\n    else:\n      return \"N\"\n# Function is registered with spark catalog\nspark.udf.register(\"parseAgeFunction\",ageCheck,StringType())\n\n# New column Adult is created by using UDF and by passing age as parameter.\ndf2 = df1.withColumn(\"adult\",expr(\"parseAgeFunction(age)\"))\ndf1.show()\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddbe0f03-54b9-42b9-8fa6-35261d301225"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Input data - List\nmyList = [(1,\"2013-07-25\",11599,\"CLOSED\"),\n          (2,\"2014-07-25\",256,\"PENDING_PAYMENT\"),\n          (3,\"2013-07-25\",11599,\"COMPLETE\"),\n          (4,\"2019-07-25\",8827,\"CLOSED\")]\n\nspark.createDataFrame(myList).show()\n\ndf1=spark.createDataFrame(myList)\\\n    .toDF(\"orderid\",\"orderdate\",\"customerid\",\"status\").show()\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c953561a-130c-4449-8b15-00776d07af63"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ndf1 = spark.read\\\n   .format(\"csv\")\\\n   .option(\"inferSchema\", True)\\\n   .option(\"path\", \"/FileStore/tables/sample_data.csv\")\\\n   .load()\n\ndf2 = df1.toDF(\"name\",\"age\",\"city\")\n\n# ageCheck User defined Function (UDF) is created.\ndef ageCheck(age):\n    if(age > 18):\n      return \"Y\"\n    else:\n      return \"N\"\n\n# Function is registered with spark catalog\nspark.udf.register(\"parseAgeFunction\",ageCheck,StringType())\n\n# New column Adult is created by using UDF and age as the parameter.\ndf3 = df2.withColumn(\"adult\",expr(\"parseAgeFunction(age)\"))\n\n# Print df3 result to console.\ndf1.show()\ndf2.show()\ndf3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74fd14c9-e84f-430e-93cd-a5788d3a9d09"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\ndf1 = spark.read\\\n   .format(\"csv\")\\\n   .option(\"inferSchema\", True)\\\n   .option(\"path\", \"/FileStore/tables/sample_data.csv\")\\\n   .load()\n\ndf1.show()\n\ndf2 = df1.toDF(\"name\",\"age\",\"city\")\n\n# New column Adult is created by using UDF and age as the parameter.\ndf2.createOrReplaceTempView(\"sam_data\")\n\ndf3 = spark.sql(\"\"\"\nSELECT \nname,\nage,\ncity,\ncase when age < 18 then 'N' else 'Y' end as adult\nFROM sam_data \"\"\")\n\n# Print df3 result to console.\ndf3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"385f27d4-f0f9-477a-9eed-20eaa393e39c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Read the data from sample_data.csv and load it to dataframe.\ndf1 = spark.read\\\n   .format(\"csv\")\\\n   .option(\"inferSchema\", True)\\\n   .option(\"path\", \"/FileStore/tables/sample_data.csv\")\\\n   .load()\n\ndf2 = df1.toDF(\"name\",\"age\",\"city\")\n\n# Create database lohith, if it is not existed.\nspark.sql(\"create database if not exists lohith\")\n\n# Create Table sample1 under the database lohith, and overwrite the data.\ndf2.write\\\n    .format(\"csv\")\\\n    .mode(\"overwrite\")\\\n    .saveAsTable(\"lohith.sample\")\n\n# OVERWRITE the data to the DBFS location \"/FileStore/tables/output\"\n# df2.write\\\n#     .format(\"csv\")\\\n#     .mode(\"overwrite\")\\\n#     .option(\"path\",\"/FileStore/tables/output\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f25de5a3-b9a4-41d4-a502-af54223bc13e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Read the data from unstructured_data.txt and load it to dataframe.\ndf1 = spark.read.text(\"/FileStore/tables/unstructured_data.txt\")\n\nprint(\"Input unstructured data\")\ndf1.show(truncate = False) # Printing to the console\n\n# Convert unstructued data to structured data using Regular expressions\nregular_expn = r'^(\\S+) (\\S+)\\t(\\S+)\\,(\\S+)'\n\ndf2 = df1.select(regexp_extract('value',regular_expn,1).alias(\"No\"),\n                 regexp_extract('value',regular_expn,2).alias(\"name\"),\n                 regexp_extract('value',regular_expn,3).alias(\"company\"),\n                 regexp_extract('value',regular_expn,4).alias(\"department\"))\n\nprint(\"output stuctured data\")\ndf2.show() # Printing to the console"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b267addc-5e9d-43a9-b069-536c3e39d880"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n# Read the data from unstructured_data.txt and load it to dataframe.\ndf1 = spark.read.text(\"/FileStore/tables/unstructured_data1.txt\")\n\nprint(\"Input unstructured data\")\ndf1.show(truncate = False) # Printing to the console\n\n# Convert unstructued data to structured data using Regular expressions\nregular_expn = r'^(\\S+)-(\\S+)-(\\S+)-(\\S+)'\n\ndf2 = df1.select(regexp_extract('value',regular_expn,1).alias(\"No\"),\n                 regexp_extract('value',regular_expn,2).alias(\"name\"),\n                 regexp_extract('value',regular_expn,3).alias(\"company\"),\n                 regexp_extract('value',regular_expn,4).alias(\"department\"))\n\nprint(\"output stuctured data\")\ndf2.show() # Printing to the console"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb342c34-9a37-4e83-962f-e209656e3dae"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10b077a2-aaee-49d1-be16-6456e201d173"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"LGK - PySpark","dashboards":[{"elements":[],"guid":"fff59d69-3ddb-4f7f-8b3c-2e07e8932235","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"aa761665-6fe3-4fa2-add2-63f38dfd77fa","origId":1726164927102263,"title":"Untitled","width":1024,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1942091552896324}},"nbformat":4,"nbformat_minor":0}
