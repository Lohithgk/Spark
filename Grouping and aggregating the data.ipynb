{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4a3fd78-2847-4bc2-8158-bad4ba70e2e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample dataset, and create a spark dataframe using it.\n",
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "              (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "              (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "              (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "              (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "              (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "              (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "              (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "              (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)]\n",
    "\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33a0cf8-d142-46ad-94c5-46722092370c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# groupBy() on department column of DataFrame, Then sum, min, max, avg, and mean on the Individual column.\n",
    "df.groupBy(\"department\").sum(\"salary\").show(truncate=False)\n",
    "df.groupBy(\"department\").min(\"salary\").show(truncate=False)\n",
    "df.groupBy(\"department\").max(\"salary\").show(truncate=False)\n",
    "df.groupBy(\"department\").avg(\"salary\").show(truncate=False)\n",
    "df.groupBy(\"department\").mean(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c0ec9b4-5fb8-4955-aef1-8b0c5f264e49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Run groupBy and aggregate on two or more DataFrame columns.\n",
    "# groupby on department, state and sum() on salary and bonus columns.\n",
    "(df\n",
    " .groupBy(\"department\",\"state\")\n",
    " .sum(\"salary\",\"bonus\")\n",
    " .show(truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5eaf8cd9-3457-4cd9-bbdc-7f6aacd4ee1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Running more aggregates at a time\n",
    "# Using agg() aggregate function we can calculate many aggregations at a time on a single statement.\n",
    "from pyspark.sql.functions import sum,avg,max,min,mean,count\n",
    "\n",
    "(df.groupBy(\"department\", \"state\")\n",
    "   .agg(\n",
    "        sum(\"salary\").alias(\"sum_salary\"),\n",
    "        avg(\"salary\").alias(\"avg_salary\"),\n",
    "        sum(\"bonus\").alias(\"sum_bonus\"),\n",
    "        max(\"bonus\").alias(\"max_bonus\"))\n",
    "   .show(truncate=False))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Grouping and aggregating the data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
